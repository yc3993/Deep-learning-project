{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "A 2 layer MLP is sufficient to model most functions. Why are deep networks used instead of 2 layer networks? \n",
    "\n",
    "   Your answer: **[Although 2 layer MLP is theoretically to model most functions, it may need a lot of neurons in the only hidden layer. Which means you need to do more calculations than deeper network and more data to help you learn the weight and bias.  ]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "What are the differences between stochastic gradient descent and Batch gradient descent? Why is mini-batch gradient descent used in practice?\n",
    "\n",
    "   Your answer: **[The main difference is that the amount of sample data calculated is different every time the parameter is updated. For BGD, each parameter update, the entire data sample set needs to be calculated, so that the speed of the batch gradient descent method will be slow. For SGD, unlike BGD, each parameter update requires calculating the gradient of the entire data set, but only one sample is selected. Mini-batch is a combination of BGD and SGD. Every time it will select a mini-batch data sample with a size of m to calculate its gradient for each parameter. \n",
    "Mini-batch gradient descent not only ensures the speed of training, but also ensures the accuracy.  ]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why are activation functions used in deep learning? What are the issues that can arise when using sigmoid or tanh activation functions?\n",
    "\n",
    "   Your answer: **[The activation functions used in deep learning is ReLU. Power operation is required when using sigmoid and tanh. So the calculation cost is higher than ReLU. ]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What are the differences between linear and logistic regression? Are both linear?\n",
    "\n",
    "   Your answer: **[Linear regression is used for regression problems, logistic regression is used for classification problems. linear regression is linear, the calculation in logistic regression is linear but after that we need to use unit-step Function to mak it a classification problem. ]**\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: **[starting: hidden_dim=400, num_classes=20, reg=1e-4, weight_scale=1e-3.  \n",
    "   \n",
    "First, I compare the last accuracy to the verification accuracy. I found the test acc of the verification is lower, so I try to add reg to 1e-3, but the output is getting worse to 0.829. \n",
    "\n",
    "Second, I increase the hidden_dim to 450, the output is getting better to 0.8303.\n",
    "\n",
    "Third, I increase the learning rate and after I increase lr to 2e-3, the result is getting worse. The best result is 0.8655 where lr is 2e-3.\n",
    "\n",
    "Fourth, I increase the num_epoch to 15, the result is increased to 0.869.\n",
    "]**\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[I adjusted the perplexity, the reason why I chose to adjust it is that perplexity means focus on nearby points or all the points. I found the origin points is not close enough, so I increase the perplexity]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
